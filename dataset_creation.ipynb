{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yburt\\anaconda3\\envs\\env_scrapping\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Value\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import re\n",
    "import unidecode\n",
    "pd.set_option('display.max_rows', 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column(data, name_col, value):\n",
    "    new_col = [value] * len(data)\n",
    "    data = data.add_column(name_col, new_col) \n",
    "    return data\n",
    "\n",
    "def print_value_count(data, column_s):\n",
    "    temp=pd.DataFrame(data[column_s], columns=[column_s])\n",
    "    print(temp.groupby(column_s).value_counts())\n",
    "\n",
    "def explode_answer(x):\n",
    "    x[\"answers\"] = x[\"answers\"].get(\"text\")\n",
    "    return x\n",
    "\n",
    "def remove_non_authorized_char(x):\n",
    "    if type(x)==x: y = x\n",
    "    else: y=str(x)\n",
    "    return \"\".join([m if m in allowed_char else \" \" for m in y]) \n",
    "\n",
    "allowed_char = \"abcdefghijklmnopqrstruvwxyz0123456789 ?,;.-!()+*_@><=:[]\"\n",
    "\n",
    "def data_preprocessing(data: pd.DataFrame, cols_clean: List[str], cols_keep: List[str]):\n",
    "\n",
    "    data_to_clean = data[cols_clean]\n",
    "\n",
    "    to_ascii= lambda x: unidecode.unidecode(x) if type(x)==str else unidecode.unidecode(str(x))\n",
    "    data_ascii=data_to_clean.fillna(\" \").map(to_ascii)\n",
    "\n",
    "    to_lower = lambda x: x.lower() if type(x)==str else str(x).lower()\n",
    "    data_lower=data_ascii.fillna(\" \").map(to_lower)\n",
    "\n",
    "    to_alphanumeric = lambda x: remove_non_authorized_char(x)\n",
    "    data_alpha = data_lower.map(to_alphanumeric)\n",
    "\n",
    "    remove_double_space = lambda x: \" \".join(x.split()) if type(x)==str else  \" \".join(str(x).split())\n",
    "    clean_data = data_alpha.map(remove_double_space)\n",
    "\n",
    "    clean_data[cols_keep] = data[cols_keep]\n",
    "\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset summary\n",
    "- None\n",
    "\n",
    "### Supported Tasks and Leaderboards\n",
    "- No specified\n",
    "\n",
    "### Use in models\n",
    "- nol2pro (fine-tuned version of t5-small on the yahoo_answers_qa dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = load_dataset(\"yahoo_answers_qa\", split = \"train\")\n",
    "dataset1_b = create_column(dataset1, \"which_data\", \"train\")\n",
    "dataset1_b = dataset1_b.filter(lambda x: (x['main_category'] == 'Business & Finance') or (x[\"main_category\"] == \"Computers & Internet\"))\n",
    "dataset1_b = create_column(dataset1_b, \"source\", \"yahoo_answers_qa\")\n",
    "dataset1_b = create_column(dataset1_b, \"type\", \"general questions finance\")\n",
    "dataset1_b = create_column(dataset1_b, \"task\", [\"question answering tasks\"])\n",
    "dataset1_b = create_column(dataset1_b, \"model\", \"nol2pro\")\n",
    "dataset1_b = create_column(dataset1_b, \"language\", \"english\")\n",
    "\n",
    "dataset1_b1= create_column(dataset1_b, \"context\", \"None\")\n",
    "dataset1_b1=dataset1_b1.rename_columns({\"context\": \"context\", 'question': 'question', \"answer\": \"answer\", \"main_category\": \"question_category\"})\n",
    "dataset1_b1= dataset1_b1.remove_columns([\"nbestanswers\", \"id\"])\n",
    "\n",
    "dataset1_b2 = dataset1_b1.map(lambda x: {\"question_category\": [x[\"question_category\"]]})\n",
    "dataset1_f = dataset1_b2.map(lambda x: {\"answer\": [x[\"answer\"]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_fp = dataset1_f.to_pandas()\n",
    "\n",
    "replace_char = lambda x: str(x).replace('&', ',')\n",
    "dataset1_fp.loc[:,\"question_category\"] = dataset1_fp.question_category.map(replace_char)\n",
    "\n",
    "cols_to_clean = [\"question\", \"answer\", \"question_category\", \"context\"]\n",
    "cols_to_keep = ['which_data', 'source', 'type', 'task', 'model', 'context']\n",
    "dataset1_fp = data_preprocessing(dataset1_fp, \n",
    "                                 cols_clean = cols_to_clean,\n",
    "                                 cols_keep= cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_fp.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\yahoo_general_question_datasets.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Summary\n",
    "\n",
    "This dataset comes originally from kaggle. It was originally split into three tables (CSV files) (Questions, Answers, and Tags) now merged into a single table. Each row corresponds to a pair (question-answer) and their associated tags.\n",
    "\n",
    "The dataset contains all questions asked between August 2, 2008 and Ocotober 19, 2016.\n",
    "\n",
    "### Supported Tasks and Leaderboards\n",
    "\n",
    "This might be useful for open-domain question-answering tasks. \n",
    "\n",
    "### Use in model\n",
    "\n",
    "- llama-2-7b-finetuned-python-qa_tokenizer (no more specifications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = load_dataset(\"koutch/stackoverflow_python\")\n",
    "dataset2_b = create_column(dataset2.get(\"train\"), \"which_data\", \"train\")\n",
    "dataset2_b = create_column(dataset2_b, \"source\", \"koutch/stackoverflow_python\")\n",
    "dataset2_b = create_column(dataset2_b, \"type\", \"technical questions\")\n",
    "dataset2_b = create_column(dataset2_b, \"task\", [\"question answering tasks\"])\n",
    "dataset2_b = create_column(dataset2_b, \"model\", \"llama-2-7b-finetuned-python-qa_tokenizer\")\n",
    "dataset2_b = create_column(dataset2_b, \"language\", \"english\")\n",
    "\n",
    "dataset2_b1=dataset2_b.rename_columns({'title': \"context\", 'question_body': 'question', \"answer_body\": \"answer\", \"tags\": \"question_category\"})\n",
    "dataset2_b1= dataset2_b1.remove_columns([\"question_score\", \"question_id\", \"question_date\", \"answer_id\", \"answer_score\", \"answer_date\"])\n",
    "dataset2_f = dataset2_b1.map(lambda x: {\"answer\": [x[\"answer\"]]})\n",
    "\n",
    "dataset2_splitted = dataset2_f.train_test_split(test_size=0.9, shuffle=False, seed=1256)\n",
    "dataset2_split_1= dataset2_splitted.get(\"train\").train_test_split(train_size=19742, shuffle=False, seed=1256)\n",
    "#dataset2_split_1.get(\"train\").to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\kaggle_dataset_part1.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funtion_dataset2(data: pd.DataFrame):\n",
    "\n",
    "    regex_pattern = r\"<p>|</p>\\n\\n<p>|<p>|\\\\r\\\\n|\\\\n|\\\\r|\\\\|</em|</p>\\n\\n<ul>\\n<li>|</li>\\n<li>|</li>\\n</ul>\\n|</p|</a>|>|<a|sips|\\|\"\n",
    "    replace_char = lambda x: re.sub(regex_pattern, \" \", str(x))\n",
    "    data.loc[:,(\"question\", \"answer\", \"context\")] = data[[\"question\", \"answer\", \"context\"]].map(replace_char)\n",
    "\n",
    "    cols_to_clean = [\"question\", \"answer\", \"context\", \"question_category\"]\n",
    "    cols_to_keep = ['which_data', 'source', 'type', 'task', 'model', 'context']\n",
    "    data = data_preprocessing(\n",
    "        data, \n",
    "        cols_clean = cols_to_clean,\n",
    "        cols_keep= cols_to_keep)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2_split_1p = dataset2_split_1.get(\"train\").to_pandas()\n",
    "dataset2_split_1p = funtion_dataset2(dataset2_split_1p)\n",
    "dataset2_split_1p.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\kaggle_dataset_part1\" +\".csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp = dataset2_split_1\n",
    "for i in range(2, 5):\n",
    "    dataset_split = data_temp.get(\"test\").train_test_split(train_size=19742, shuffle=False, seed=1256)\n",
    "    data_temp = dataset_split\n",
    "    data = dataset_split.get(\"train\").to_pandas()\n",
    "    data = funtion_dataset2(data)\n",
    "    data.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\kaggle_dataset_part\" + str(i) +\".csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2_split_5 = dataset2_splitted.get(\"test\").train_test_split(train_size=19742, shuffle=False, seed=1256)\n",
    "dataset2_split_5p = dataset2_split_5.get(\"train\").to_pandas()\n",
    "dataset2_split_5p = funtion_dataset2(dataset2_split_5p)\n",
    "dataset2_split_5p.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\kaggle_dataset_part5.csv\", sep=\"|\")\n",
    "\n",
    "data_temp = dataset2_split_5\n",
    "for i in range(1, 45):\n",
    "    dataset_split = data_temp.get(\"test\").train_test_split(train_size=19742, shuffle=False, seed=1256)\n",
    "    data_temp = dataset_split\n",
    "    data = dataset_split.get(\"train\").to_pandas()\n",
    "    data = funtion_dataset2(data)\n",
    "    data.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\kaggle_dataset_part\" + str(i + 4) +\".csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQuAD-fr:\n",
    "\n",
    "- a translated version of the Stanford Question Answering Dataset (SQuAD) into French obtained through automatic translation of the English dataset\n",
    "- a reading comprehension dataset, consisting of approximately 90K factoid questions on Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage serves as a means of data augmentation on FQuAD and PIAF benchmarks\n",
    "\n",
    "\n",
    "#### Supported Tasks and Leaderboards\n",
    "closed-domain-qa, text-retrieval: This dataset is intended to be used for closed-domain-qa, but can also be used for information retrieval tasks.\n",
    "\n",
    "### Use in models\n",
    "\n",
    "- No communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = load_dataset(\"qwant/squad_fr\")\n",
    "dataset3 = dataset3.map(explode_answer)\n",
    "\n",
    "dataset3_b = create_column(dataset3.get(\"train\"), \"which_data\", \"train\")\n",
    "dataset3_b = create_column(dataset3_b, \"source\", \"qwant/squad_fr\")\n",
    "dataset3_b = create_column(dataset3_b, \"type\", \"general questions\")\n",
    "dataset3_b = create_column(dataset3_b, \"task\", [\"question answering tasks\", \"text-retrieval\"])\n",
    "dataset3_b = create_column(dataset3_b, \"model\", \"no communication\")\n",
    "dataset3_b = create_column(dataset3_b, \"language\", \"french\")\n",
    "\n",
    "dataset3_b=dataset3_b.filter(lambda example: (example['title']== 'Energy')\n",
    "                 or (example[\"title\"]==\"Communication\")\n",
    "                 or (example[\"title\"]==\"Computer\")\n",
    "                 or (example[\"title\"]==\"Computer_security\")\n",
    "                 or (example[\"title\"]==\"Economic_inequality\")\n",
    "                 or (example[\"title\"]==\"European_Union_law\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3_t = create_column(dataset3.get(\"validation\"), \"which_data\", \"validation\")\n",
    "dataset3_t = create_column(dataset3_t, \"source\", \"qwant/squad_fr\")\n",
    "dataset3_t = create_column(dataset3_t, \"type\", \"general questions\")\n",
    "dataset3_t = create_column(dataset3_t, \"task\", [\"question answering tasks\", \"text-retrieval\"])\n",
    "dataset3_t = create_column(dataset3_t, \"model\", \"no communication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3_c = concatenate_datasets([dataset3_t, dataset3_b])\n",
    "dataset3_c = create_column(dataset3_c, \"question_category\", \"None\")\n",
    "dataset3_c = dataset3_c.rename_columns({\"context\": \"context\", 'question': 'question', \"answers\": \"answer\"})\n",
    "dataset3_c = dataset3_c.remove_columns([\"title\", \"id\"])\n",
    "dataset3_f = dataset3_c.map(lambda x: {\"question_category\": [x[\"question_category\"]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3_fp = dataset3_f.to_pandas()\n",
    "cols_to_clean = [\"question\", \"answer\", \"question_category\", \"context\"]\n",
    "cols_to_keep = ['which_data', 'source', 'type', 'task', 'model', 'context']\n",
    "dataset3_fp = data_preprocessing(dataset3_fp, \n",
    "                                 cols_clean = cols_to_clean,\n",
    "                                 cols_keep= cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3_fp.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\standord_translated_dataset.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "### Dataset Summary\n",
    "We are thrilled to announce the release of the OpenOrca dataset! This rich collection of augmented FLAN data aligns, as best as possible, with the distributions outlined in the Orca paper. It has been instrumental in generating high-performing model checkpoints and serves as a valuable resource for all NLP researchers and developers!\n",
    "\n",
    "### Supported Tasks and Leaderboards\n",
    "\n",
    "This dataset supports a range of tasks including language modeling, text generation, and text augmentation. It has been instrumental in the generation of multiple high-performing model checkpoints which have exhibited exceptional performance in our unit testing. \n",
    "\n",
    "### Use in model\n",
    "\n",
    "- Fine tuning on top of Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4 = load_dataset(\"Open-Orca/OpenOrca\")\n",
    "dataset4_b = create_column(dataset4.get(\"train\"), \"which_data\", \"train\")\n",
    "dataset4_b = create_column(dataset4_b, \"source\", \"Open-Orca/OpenOrca\")\n",
    "dataset4_b = create_column(dataset4_b, \"type\", \"general questions\")\n",
    "dataset4_b = create_column(dataset4_b, \"task\", [\"language modeling\", \"text generation\", \"text augmentation\"])\n",
    "dataset4_b = create_column(dataset4_b, \"model\", \"fine tuned mistral 7B\")\n",
    "dataset4_b = create_column(dataset4_b, \"language\", \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4_b1=dataset4_b.rename_columns({\"system_prompt\": \"context\", 'question': 'question', \"response\": \"answer\"})\n",
    "dataset4_b1= dataset4_b1.remove_columns([\"id\"])\n",
    "dataset4_b1 = create_column(dataset4_b1, \"question_category\", \"None\")\n",
    "\n",
    "dataset4_b2 = dataset4_b1.map(lambda x: {\"question_category\": [x[\"question_category\"]]})\n",
    "dataset4_f = dataset4_b2.map(lambda x: {\"answer\": [x[\"answer\"]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funtion_dataset4(data: pd.DataFrame):\n",
    "\n",
    "    regex_pattern = r\"[\\n|\\n]|\\n\"\n",
    "    replace_char = lambda x: re.sub(regex_pattern, \" \", str(x))\n",
    "    data.loc[:,(\"question\", \"answer\")] = data[[\"question\", \"answer\"]].map(replace_char)\n",
    "\n",
    "    cols_to_clean = [\"question\", \"answer\", \"context\", \"question_category\"]\n",
    "    cols_to_keep = ['which_data', 'source', 'type', 'task', 'model', 'context']\n",
    "    data = data_preprocessing(\n",
    "        data, \n",
    "        cols_clean = cols_to_clean,\n",
    "        cols_keep= cols_to_keep)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4_splitted = dataset4_f.train_test_split(test_size=0.9, shuffle=False, seed=1256)\n",
    "dataset4_split_1= dataset4_splitted.get(\"train\").train_test_split(train_size=19742, shuffle=False, seed=1256)\n",
    "\n",
    "dataset4_split_1p = dataset4_split_1.get(\"train\").to_pandas()\n",
    "dataset4_split_1p = funtion_dataset4(dataset4_split_1p)\n",
    "dataset4_split_1p.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\mistral_replica_part1.csv\", sep=\"|\")\n",
    "\n",
    "data_temp = dataset4_split_1\n",
    "for i in range(2, 5):\n",
    "    dataset_split = data_temp.get(\"test\").train_test_split(train_size=19742, shuffle=False, seed=1256)\n",
    "    data_temp = dataset_split\n",
    "    dataset_splitp = dataset_split.get(\"train\").to_pandas()\n",
    "    dataset_splitp = funtion_dataset4(dataset_splitp)\n",
    "    dataset_splitp.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\mistral_replica_part\" + str(i) +\".csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4_split_5 = dataset4_splitted.get(\"test\").train_test_split(train_size=19742, shuffle=False, seed=1256)\n",
    "dataset4_split_5p = dataset4_split_5.get(\"train\").to_pandas()\n",
    "dataset4_split_5p = funtion_dataset4(dataset4_split_5p)\n",
    "dataset4_split_5p.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\mistral_replica_part5.csv\", sep=\"|\")\n",
    "\n",
    "data_temp = dataset4_split_5\n",
    "for i in range(1, 100):\n",
    "    dataset_split = data_temp.get(\"test\").train_test_split(train_size=19742, shuffle=False, seed=1256)\n",
    "    data_temp = dataset_split\n",
    "    dataset_splitp = dataset_split.get(\"train\").to_pandas()\n",
    "    dataset_splitp = funtion_dataset4(dataset_splitp)\n",
    "    dataset_splitp.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\mistral_replica_part\" + str(i + 4) +\".csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------\n",
    "### Dataset Summary\n",
    "\n",
    "FinTalk-19k is a domain-specific dataset designed for the fine-tuning of Large Language Models (LLMs) with a focus on financial conversations. Extracted from public Reddit conversations, this dataset is tagged with categories like \"Personal Finance\", \"Financial Information\", and \"Public Sentiment\". It consists of more than 19,000 entries, each representing a conversation about financial topics.\n",
    "\n",
    "### Supported Tasks and Leaderboards\n",
    "\n",
    "- language-modeling: The dataset can be used to train models for language modeling in the context of financial discussions.\n",
    "- text-generation: Suitable for generating responses in financial conversations.\n",
    "\n",
    "### Languages\n",
    "\n",
    "The dataset is primarily in English.\n",
    "\n",
    "### Sources: from REDDIT\n",
    "\n",
    "### Use in model\n",
    "- No communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset5 = load_dataset(\"ceadar-ie/FinTalk-19k\")\n",
    "dataset5_b = create_column(dataset5.get(\"train\"), \"which_data\", \"train\")\n",
    "dataset5_b = create_column(dataset5_b, \"source\", \"ceadar-ie/FinTalk-19k\")\n",
    "dataset5_b = create_column(dataset5_b, \"type\", \"general financial questions\")\n",
    "dataset5_b = create_column(dataset5_b, \"task\", [\"question answering tasks\"])\n",
    "dataset5_b = create_column(dataset5_b, \"model\", \"no communication\")\n",
    "dataset5_b = create_column(dataset5_b, \"language\", \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset5_b1=dataset5_b.rename_columns({\"context\": \"context\", 'instruction': 'question', \"response\": \"answer\", \"tag\":\"question_category\"})\n",
    "dataset5_b2 = dataset5_b1.map(lambda x: {\"question_category\": [x[\"question_category\"]]})\n",
    "dataset5_f = dataset5_b2.map(lambda x: {\"answer\": [x[\"answer\"]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset5_fp = dataset5_f.to_pandas()\n",
    "dataset5_fp = data_preprocessing(dataset5_fp, \n",
    "                                 cols_clean = [\"question\", \"answer\", \"context\", \"question_category\"],\n",
    "                                 cols_keep= [\"which_data\", \"source\", \"type\", \"task\", \"model\", \"language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset5_fp.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\reddit_technical.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------\n",
    "### Dataset Summary\n",
    "\n",
    "The x-stance dataset contains more than 150 political questions, and 67k comments written by candidates on those questions. The comments are partly German, partly French and Italian. The data have been extracted from the Swiss voting advice platform Smartvote.\n",
    "\n",
    "### Supported Tasks and Leaderboards\n",
    "\n",
    "- fast checking\n",
    "\n",
    "### Model in use\n",
    "-  mDeBERTa-v3-base-tasksource-nli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset6 = load_dataset(\"strombergnlp/x-stance\", \"fr\")\n",
    "dataset6_b = create_column(dataset6.get(\"validation\"), \"which_data\", \"validation\")\n",
    "dataset6_b = create_column(dataset6_b, \"source\", \"strombergnlp/x-stance\")\n",
    "dataset6_b = create_column(dataset6_b, \"type\", \"political questions\")\n",
    "dataset6_b = create_column(dataset6_b, \"task\", [\"question answering tasks\"])\n",
    "dataset6_b = create_column(dataset6_b, \"model\", \"mDeBERTa-v3-base-tasksource-nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset6_t = create_column(dataset6.get(\"test\"), \"which_data\", \"test\")\n",
    "dataset6_t = create_column(dataset6_t, \"source\", \"strombergnlp/x-stance\")\n",
    "dataset6_t = create_column(dataset6_t, \"type\", \"political questions\")\n",
    "dataset6_t = create_column(dataset6_t, \"task\", [\"question answering tasks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset6_q = create_column(dataset6.get(\"train\"), \"which_data\", \"train\")\n",
    "dataset6_q = create_column(dataset6_q, \"source\", \"strombergnlp/x-stance\")\n",
    "dataset6_q = create_column(dataset6_q, \"type\", \"political questions\")\n",
    "dataset6_q = create_column(dataset6_q, \"task\", [\"question answering tasks\"])\n",
    "dataset6_q = create_column(dataset6_q, \"model\", \"mDeBERTa-v3-base-tasksource-nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset6_c = concatenate_datasets([dataset6_q, dataset6_t, dataset6_b])\n",
    "dataset6_c = create_column(dataset6_c, \"question_category\", \"None\")\n",
    "dataset6_c = create_column(dataset6_c, \"language\", \"french\")\n",
    "dataset6_c = dataset6_c.rename_columns({'label': \"context\", 'question': 'question', \"comment\": \"answer\"})\n",
    "dataset6_c = dataset6_c.remove_columns([\"id\"])\n",
    "\n",
    "dataset6_c1 = dataset6_c.map(lambda x: {\"question_category\": [x[\"question_category\"]]})\n",
    "dataset6_c2 = dataset6_c1.map(lambda x: {\"answer\": [x[\"answer\"]]})\n",
    "dataset6_f = dataset6_c2.cast_column('context', Value(dtype='string', id=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset6_f = dataset6_f.to_pandas()\n",
    "dataset6_f = data_preprocessing(dataset6_f, \n",
    "                                 cols_clean = [\"question\", \"answer\", \"context\", \"question_category\"],\n",
    "                                 cols_keep= [\"which_data\", \"source\", \"type\", \"task\", \"model\", \"language\"])\n",
    "dataset6_f.to_csv(\"C:\\\\Users\\\\yburt\\\\Documents\\\\dataset_tests\\\\political_questions.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_scrapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
